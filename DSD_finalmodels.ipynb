{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustStorItemTriadID</th>\n",
       "      <th>BaseorderID</th>\n",
       "      <th>Createdate</th>\n",
       "      <th>RecDeliveryDate</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>OaMasterDistributorID</th>\n",
       "      <th>ConversionFactor</th>\n",
       "      <th>InventoryChange</th>\n",
       "      <th>ActualScans</th>\n",
       "      <th>ForecastedScans</th>\n",
       "      <th>...</th>\n",
       "      <th>twoagoqty_avg</th>\n",
       "      <th>OGvsModifiedOG</th>\n",
       "      <th>lastdeliv_create_overlap</th>\n",
       "      <th>daily_invntchange</th>\n",
       "      <th>avg_invent_change</th>\n",
       "      <th>current_vs_avg_invnratio</th>\n",
       "      <th>days_last_delivered</th>\n",
       "      <th>forecasted_origprop_ratio</th>\n",
       "      <th>trup_origprop_ratio</th>\n",
       "      <th>actual_origprop_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13761530</td>\n",
       "      <td>1867926</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.940860</td>\n",
       "      <td>1.020110</td>\n",
       "      <td>4</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13766672</td>\n",
       "      <td>1862453</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.735394</td>\n",
       "      <td>0.906543</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13771739</td>\n",
       "      <td>1867825</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049231</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>9.110215</td>\n",
       "      <td>1.015344</td>\n",
       "      <td>4</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13772017</td>\n",
       "      <td>1860527</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435185</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.317204</td>\n",
       "      <td>1.294664</td>\n",
       "      <td>3</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13774001</td>\n",
       "      <td>1863282</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustStorItemTriadID  BaseorderID  Createdate RecDeliveryDate  CategoryID  \\\n",
       "0             13761530      1867926  2020-05-01      2020-05-07           1   \n",
       "1             13766672      1862453  2020-05-01      2020-05-06           1   \n",
       "2             13771739      1867825  2020-05-01      2020-05-07           1   \n",
       "3             13772017      1860527  2020-05-01      2020-05-05           1   \n",
       "4             13774001      1863282  2020-05-01      2020-05-05           1   \n",
       "\n",
       "   OaMasterDistributorID  ConversionFactor  InventoryChange  ActualScans  \\\n",
       "0                      5              0.75               12            0   \n",
       "1                      5              0.75                4            0   \n",
       "2                      5              0.75               37            0   \n",
       "3                      5              0.75                9            0   \n",
       "4                      5              0.75                0            0   \n",
       "\n",
       "   ForecastedScans  ... twoagoqty_avg  OGvsModifiedOG  \\\n",
       "0               13  ...      0.620000               1   \n",
       "1                4  ...      0.000000               1   \n",
       "2               43  ...      1.049231               1   \n",
       "3               18  ...      1.435185               1   \n",
       "4                0  ...      0.000000               0   \n",
       "\n",
       "   lastdeliv_create_overlap  daily_invntchange  avg_invent_change  \\\n",
       "0                      -3.0           3.000000           2.940860   \n",
       "1                      -1.0           0.666667           0.735394   \n",
       "2                      -3.0           9.250000           9.110215   \n",
       "3                       0.0           3.000000           2.317204   \n",
       "4                       1.0           0.000000           0.077942   \n",
       "\n",
       "   current_vs_avg_invnratio  days_last_delivered  forecasted_origprop_ratio  \\\n",
       "0                  1.020110                    4                   1.444444   \n",
       "1                  0.906543                    6                   0.000000   \n",
       "2                  1.015344                    4                   0.895833   \n",
       "3                  1.294664                    3                   1.125000   \n",
       "4                  0.000000                   10                   0.000000   \n",
       "\n",
       "   trup_origprop_ratio  actual_origprop_ratio  \n",
       "0             0.000000                    0.0  \n",
       "1             0.000000                    0.0  \n",
       "2             0.041667                    0.0  \n",
       "3             0.000000                    0.0  \n",
       "4             0.000000                    0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will read the dataset that we created in the previous exercise\n",
    "df = pd.read_csv(\"DSDPartners_Data.csv\", encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also try Operator Adjustments as potential target\n",
    "target = 'PropOrderQty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(df[target])\n",
    "#y = np.reshape(y,(y.shape[0],1))\n",
    "X = df.drop(['CustStorItemTriadID','BaseorderID','Createdate','ModelUsed','RecDeliveryDate',\n",
    "             'ConversionFactor','Previous2DelDate','MaxScanDate','MaxShipDate','Reviewed','IncInAnom',\n",
    "            'PrevDeliveryDate'], axis = 1).drop(target, axis=1).fillna(0)\n",
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>OaMasterDistributorID</th>\n",
       "      <th>InventoryChange</th>\n",
       "      <th>ActualScans</th>\n",
       "      <th>ForecastedScans</th>\n",
       "      <th>WeightData</th>\n",
       "      <th>BaseOrder</th>\n",
       "      <th>SlowMoving</th>\n",
       "      <th>TooBig</th>\n",
       "      <th>TrueUpAdjQty</th>\n",
       "      <th>...</th>\n",
       "      <th>twoagoqty_avg</th>\n",
       "      <th>OGvsModifiedOG</th>\n",
       "      <th>lastdeliv_create_overlap</th>\n",
       "      <th>daily_invntchange</th>\n",
       "      <th>avg_invent_change</th>\n",
       "      <th>current_vs_avg_invnratio</th>\n",
       "      <th>days_last_delivered</th>\n",
       "      <th>forecasted_origprop_ratio</th>\n",
       "      <th>trup_origprop_ratio</th>\n",
       "      <th>actual_origprop_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.940860</td>\n",
       "      <td>1.020110</td>\n",
       "      <td>4</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.735394</td>\n",
       "      <td>0.906543</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049231</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>9.110215</td>\n",
       "      <td>1.015344</td>\n",
       "      <td>4</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435185</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.317204</td>\n",
       "      <td>1.294664</td>\n",
       "      <td>3</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID  OaMasterDistributorID  InventoryChange  ActualScans  \\\n",
       "0           1                      5               12            0   \n",
       "1           1                      5                4            0   \n",
       "2           1                      5               37            0   \n",
       "3           1                      5                9            0   \n",
       "4           1                      5                0            0   \n",
       "\n",
       "   ForecastedScans  WeightData  BaseOrder  SlowMoving  TooBig  TrueUpAdjQty  \\\n",
       "0               13           1          8           0       0             0   \n",
       "1                4           0          0           0       0             0   \n",
       "2               43           1         45           0       0             2   \n",
       "3               18           0         16           0       0             0   \n",
       "4                0          -2          0           0       0             0   \n",
       "\n",
       "   ...  twoagoqty_avg  OGvsModifiedOG  lastdeliv_create_overlap  \\\n",
       "0  ...       0.620000               1                      -3.0   \n",
       "1  ...       0.000000               1                      -1.0   \n",
       "2  ...       1.049231               1                      -3.0   \n",
       "3  ...       1.435185               1                       0.0   \n",
       "4  ...       0.000000               0                       1.0   \n",
       "\n",
       "   daily_invntchange  avg_invent_change  current_vs_avg_invnratio  \\\n",
       "0           3.000000           2.940860                  1.020110   \n",
       "1           0.666667           0.735394                  0.906543   \n",
       "2           9.250000           9.110215                  1.015344   \n",
       "3           3.000000           2.317204                  1.294664   \n",
       "4           0.000000           0.077942                  0.000000   \n",
       "\n",
       "   days_last_delivered  forecasted_origprop_ratio  trup_origprop_ratio  \\\n",
       "0                    4                   1.444444             0.000000   \n",
       "1                    6                   0.000000             0.000000   \n",
       "2                    4                   0.895833             0.041667   \n",
       "3                    3                   1.125000             0.000000   \n",
       "4                   10                   0.000000             0.000000   \n",
       "\n",
       "   actual_origprop_ratio  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247801, 247801, 247801, 247801)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Establish training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=314)\n",
    "\n",
    "len(X_train), len(X_test), len(y_test), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 0.71\n",
      "Mean squared error = 2.28\n",
      "Median absolute error = 0.19\n",
      "Explain variance score = 0.95\n",
      "R2 score = 0.95\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_pred), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_pred), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_pred), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('measured')\n",
    "ax.set_ylabel('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00591\n",
      "Feature: 1, Score: 0.11521\n",
      "Feature: 2, Score: 0.00014\n",
      "Feature: 3, Score: 0.19162\n",
      "Feature: 4, Score: 0.17050\n",
      "Feature: 5, Score: 0.56338\n",
      "Feature: 6, Score: 0.69091\n",
      "Feature: 7, Score: 5.97624\n",
      "Feature: 8, Score: -2.05624\n",
      "Feature: 9, Score: 0.55653\n",
      "Feature: 10, Score: 0.02746\n",
      "Feature: 11, Score: -0.06344\n",
      "Feature: 12, Score: -0.00204\n",
      "Feature: 13, Score: -0.00143\n",
      "Feature: 14, Score: -0.09032\n",
      "Feature: 15, Score: 0.05485\n",
      "Feature: 16, Score: 0.00671\n",
      "Feature: 17, Score: 0.02799\n",
      "Feature: 18, Score: 0.02727\n",
      "Feature: 19, Score: -0.21249\n",
      "Feature: 20, Score: -0.23489\n",
      "Feature: 21, Score: 0.13295\n",
      "Feature: 22, Score: -0.09208\n",
      "Feature: 23, Score: -0.32303\n",
      "Feature: 24, Score: 0.41756\n",
      "Feature: 25, Score: 0.00436\n",
      "Feature: 26, Score: 0.00320\n",
      "Feature: 27, Score: -0.01223\n",
      "Feature: 28, Score: 0.80080\n",
      "Feature: 29, Score: -0.00173\n",
      "Feature: 30, Score: 0.05445\n",
      "Feature: 31, Score: 0.36919\n",
      "Feature: 32, Score: -0.00803\n",
      "Feature: 33, Score: 0.00085\n",
      "Feature: 34, Score: 0.61534\n",
      "Feature: 35, Score: 0.82346\n",
      "Feature: 36, Score: -0.03567\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOAElEQVR4nO3df6hk5X3H8c+n69qkKrVhByOu22va0CKSGju1lYi1pglrLLUptmhJSFrKTUsMBlrStVKSFAK2pWmEloZbNRFiYsVka4j54YYYbP7IJnd1NatrU2tXohj3SpAohcjqp3/MWbhe7907e8/jzHx33y+43DNnzj7ny7N3PvPMM8/McRIBAOr6qWkXAADohyAHgOIIcgAojiAHgOIIcgAo7oRpnHTLli2Zm5ubxqkBoKw9e/Y8k2Swcv9Ugnxubk6Li4vTODUAlGX78dX2M7UCAMUR5ABQHEEOAMUR5ABQXJMgt32q7TtsP2J7v+0LWrQLAFhfq1UrN0j6apIrbJ8o6WcatQsAWEfvILf9s5IukvReSUrygqQX+rYLABhPi6mVsyQtSfqU7ftt32j7pAbtAgDG0GJq5QRJ50n6QJLdtm+QtEPS3yw/yPa8pHlJ2rZtW4PTHtvmdty15n0Hrr9sgpUAmHUtRuRPSHoiye7u9h0aBfvLJFlIMkwyHAxe8QlTAMAG9Q7yJD+U9APbv9Ttequkh/u2CwAYT6tVKx+QdGu3YuUxSX/cqF0AwDqaBHmSvZKGLdoCABwdPtkJAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQXJMLS9g+IOk5SS9KOpSEi0wAwIS0utSbJP1WkmcatgcAGANTKwBQXKsgj6S7be+xPb/aAbbnbS/aXlxaWmp0WgBAqyC/MMl5ki6V9H7bF608IMlCkmGS4WAwaHRaAECTIE/yZPf7oKSdks5v0S4AYH29g9z2SbZPObwt6e2S9vVtFwAwnharVk6TtNP24fY+m+SrDdoFAIyhd5AneUzSrzSoBQCwASw/BIDiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKK5ZkNveZPt+219q1SYAYH0tR+TXSNrfsD0AwBiaBLntrZIuk3Rji/YAAONrNSL/hKQPSXpprQNsz9tetL24tLTU6LQAgN5Bbvt3JB1MsudIxyVZSDJMMhwMBn1PCwDotBiRv0XS79o+IOk2SZfY/kyDdgEAY+gd5EmuTbI1yZykKyV9I8m7elcGABgL68gBoLgTWjaW5JuSvtmyTQDAkTEiB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiCHIAKI4gB4DiWlyz8zW2v2P7AdsP2f5oi8IAAONpcWGJn0i6JMnztjdL+pbtryT5doO2AQDr6B3kSSLp+e7m5u4nfdsFAIynyRy57U2290o6KGlXkt2rHDNve9H24tLSUovTAgDUKMiTvJjkXElbJZ1v+5xVjllIMkwyHAwGLU4LAFDjVStJnpV0j6TtLdsFAKytxaqVge1Tu+3XSnqbpEf6tgsAGE+LVSunS7rF9iaNnhhuT/KlBu0CAMbQYtXKg5Le3KAWAMAG8MlOACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4ghyACiOIAeA4lpc6u1M2/fYftj2Q7avaVEYAGA8LS71dkjSXyS5z/YpkvbY3pXk4QZtAwDW0XtEnuSpJPd1289J2i/pjL7tAgDG03SO3PacRtfv3L3KffO2F20vLi0ttTwtABzXmgW57ZMlfV7SB5P8eOX9SRaSDJMMB4NBq9MCwHGvSZDb3qxRiN+a5Ast2gQAjKfFqhVLuknS/iQf718SAOBotBiRv0XSuyVdYntv9/OOBu0CAMbQe/lhkm9JcoNaAAAbwCc7AaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAiiPIAaA4ghwAimt1zc6bbR+0va9FewCA8bUakX9a0vZGbQEAjkKTIE9yr6QftWgLAHB0JjZHbnve9qLtxaWlpUmdFgCOeRML8iQLSYZJhoPBYFKnBYBjHqtWAKA4ghwAijuhRSO2PyfpYklbbD8h6cNJbmrRNgDMmrkdd626/8D1l024kpEmQZ7kqhbtAKht1gLueMHUCgAU12REjqPHyAVAK4zIAaA4RuQAsMxar5al2X3FTJC/Spg6ATApBPkxjCcT4PjAHDkAFEeQA0BxTK0Axwim0o5fBDmAUnjCeiWmVgCgOIIcAIojyAGgOIIcAIrjzc5VrPcR3Yof4QVw7Gp1YYntkm6QtEnSjUmub9HuRhCymEWstMCrqffUiu1Nkv5F0qWSzpZ0le2z+7YLABhPixH5+ZIeTfKYJNm+TdLlkh5u0DaOc7zCOr7w/70xTtKvAfsKSduT/Gl3+92Sfj3J1SuOm5c0L0nbtm371ccff7zXeY93k/iDP1bO0dc4NfadOqnwvsws1DCOWZjGerX6yvaeJMOV+yf2ZmeSBUkLkjQcDvs9ewCYuFkK6yOZhTonXUOLIH9S0pnLbm/t9qG4WXhAAFhfiyD/rqQ32j5LowC/UtIfNWgXxwGeLEboB/TRO8iTHLJ9taSvabT88OYkD/WuDAAwliZz5Em+LOnLLdoCABwdPtkJrGOcaQ+mRjBNfNcKABTHiBwogBE/joQROQAUR5ADQHEEOQAUR5ADQHEEOQAUR5ADQHEEOQAUR5ADQHEEOQAUR5ADQHEEOQAUR5ADQHEEOQAU1yvIbf+B7Ydsv2T7FVd2BgC8+vqOyPdJ+n1J9zaoBQCwAb2+jzzJfkmy3aYaAMBRm9gcue1524u2F5eWliZ1WgA45q07Irf9dUmvX+Wu65LcOe6JkixIWpCk4XCYsSsEABzRukGe5LcnUQgAYGNYfggAxfVdfvhO209IukDSXba/1qYsAMC4+q5a2SlpZ6NaAAAbwNQKABRHkANAcQQ5ABRHkANAcQQ5ABRHkANAcQQ5ABRHkANAcQQ5ABRHkANAcQQ5ABRHkANAcQQ5ABRHkANAcQQ5ABTX98IS/2D7EdsP2t5p+9RGdQEAxtR3RL5L0jlJ3iTp+5Ku7V8SAOBo9AryJHcnOdTd/Lakrf1LAgAcjZZz5H8i6SsN2wMAjGHda3ba/rqk169y13VJ7uyOuU7SIUm3HqGdeUnzkrRt27YNFQsAeCUn6deA/V5J75P01iT/N86/GQ6HWVxc7HVeADje2N6TZLhy/7oj8nUa3S7pQ5J+c9wQBwC01XeO/J8lnSJpl+29tj/ZoCYAwFHoNSJP8outCgEAbAyf7ASA4ghyACiOIAeA4ghyACiOIAeA4ghyACiu9yc7N3RSe0nS4w2a2iLpmQbtvNoq1FmhRqlGndTYToU6J1njzycZrNw5lSBvxfbiah9XnTUV6qxQo1SjTmpsp0Kds1AjUysAUBxBDgDFVQ/yhWkXMKYKdVaoUapRJzW2U6HOqddYeo4cAFB/RA4Axz2CHACKKxvktrfb/i/bj9reMe16VmP7gO3vdd/VPjOXRLJ9s+2Dtvct2/c627ts/3f3++dmsMaP2H6y68+9tt8x5RrPtH2P7YdtP2T7mm7/rPXlWnXOTH/afo3t79h+oKvxo93+s2zv7h7n/277xBms8dO2/3dZP5478eKSlPuRtEnS/0h6g6QTJT0g6exp17VKnQckbZl2HavUdZGk8yTtW7bv7yXt6LZ3SPq7GazxI5L+ctr9t6ye0yWd122fIun7ks6ewb5cq86Z6U9JlnRyt71Z0m5JvyHpdklXdvs/KenPZ7DGT0u6Ypr9V3VEfr6kR5M8luQFSbdJunzKNZWR5F5JP1qx+3JJt3Tbt0j6vUnWtNIaNc6UJE8lua/bfk7SfklnaPb6cq06Z0ZGnu9ubu5+IukSSXd0+6fal0eoceqqBvkZkn6w7PYTmrE/zE4k3W17j+35aRezjtOSPNVt/1DSadMs5giutv1gN/Uy1SmL5WzPSXqzRqO0me3LFXVKM9SftjfZ3ivpoKRdGr3qfjbJoe6QqT/OV9aY5HA/fqzrx3+y/dOTrqtqkFdxYZLzJF0q6f22L5p2QePI6LXjTIw0VvhXSb8g6VxJT0n6x6lW07F9sqTPS/pgkh8vv2+W+nKVOmeqP5O8mORcSVs1etX9y9OsZzUra7R9jqRrNar11yS9TtJfTbquqkH+pKQzl93e2u2bKUme7H4flLRToz/OWfW07dMlqft9cMr1vEKSp7sH0kuS/k0z0J+2N2sUjrcm+UK3e+b6crU6Z7E/JSnJs5LukXSBpFNtH7628Mw8zpfVuL2bukqSn0j6lKbQj1WD/LuS3ti9o32ipCslfXHKNb2M7ZNsn3J4W9LbJe078r+aqi9Kek+3/R5Jd06xllUdDsfOOzXl/rRtSTdJ2p/k48vumqm+XKvOWepP2wPbp3bbr5X0No3m8u+RdEV32FT7co0aH1n2pG2N5vAn3o9lP9nZLZX6hEYrWG5O8rHpVvRytt+g0Shckk6Q9NlZqdH25yRdrNHXbz4t6cOS/kOjFQLbNPqK4T9MMrU3G9eo8WKNpgGi0Yqg9y2bi5442xdK+k9J35P0Urf7rzWaf56lvlyrzqs0I/1p+00avZm5SaMB5u1J/rZ7HN2m0ZTF/ZLe1Y18Z6nGb0gaaLSqZa+kP1v2puhkaqsa5ACAkapTKwCADkEOAMUR5ABQHEEOAMUR5ABQHEEOAMUR5ABQ3P8Dv7epbT96ei8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Determine which features are most important to the model\n",
    "import matplotlib.pyplot as pyplot\n",
    "importance = regr.coef_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CategoryID', 'OaMasterDistributorID', 'InventoryChange', 'ActualScans',\n",
       "       'ForecastedScans', 'WeightData', 'BaseOrder', 'SlowMoving', 'TooBig',\n",
       "       'TrueUpAdjQty', 'TUAIssue', 'OriginalPropOrderQty', 'ConversionUnits',\n",
       "       'MaxDeliveredQty', 'ConversionResidual', 'TwoOrderAgo',\n",
       "       'QtyShippedTwoOrderAgo', 'OneOrderAgo', 'lead_time', 'new_client',\n",
       "       'skipped_ship', 'never_shipped', 'lastorder_maxorder',\n",
       "       'twoago_order_maxorder', 'order_maxorder', 'originalqty_avg',\n",
       "       'lastqty_avg', 'twoagoqty_avg', 'OGvsModifiedOG',\n",
       "       'lastdeliv_create_overlap', 'daily_invntchange', 'avg_invent_change',\n",
       "       'current_vs_avg_invnratio', 'days_last_delivered',\n",
       "       'forecasted_origprop_ratio', 'trup_origprop_ratio',\n",
       "       'actual_origprop_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24781/24781 - 18s - loss: 2.8811 - val_loss: 2.0104\n",
      "Epoch 2/100\n",
      "24781/24781 - 17s - loss: 2.0066 - val_loss: 1.7931\n",
      "Epoch 3/100\n",
      "24781/24781 - 17s - loss: 1.8749 - val_loss: 1.7449\n",
      "Epoch 4/100\n",
      "24781/24781 - 17s - loss: 1.8179 - val_loss: 1.7970\n",
      "Epoch 5/100\n",
      "24781/24781 - 17s - loss: 1.7714 - val_loss: 1.7237\n",
      "Epoch 6/100\n",
      "24781/24781 - 17s - loss: 1.7614 - val_loss: 1.8228\n",
      "Epoch 7/100\n",
      "24781/24781 - 17s - loss: 1.7376 - val_loss: 1.6750\n",
      "Epoch 8/100\n",
      "24781/24781 - 17s - loss: 1.7270 - val_loss: 1.7174\n",
      "Epoch 9/100\n",
      "24781/24781 - 17s - loss: 1.7116 - val_loss: 1.7660\n",
      "Epoch 10/100\n",
      "24781/24781 - 17s - loss: 1.7067 - val_loss: 1.6687\n",
      "Epoch 11/100\n",
      "24781/24781 - 17s - loss: 1.6982 - val_loss: 1.6550\n",
      "Epoch 12/100\n",
      "24781/24781 - 17s - loss: 1.6929 - val_loss: 1.6923\n",
      "Epoch 13/100\n",
      "24781/24781 - 17s - loss: 1.6767 - val_loss: 1.6891\n",
      "Epoch 14/100\n",
      "24781/24781 - 17s - loss: 1.6700 - val_loss: 1.8201\n",
      "Epoch 15/100\n",
      "24781/24781 - 17s - loss: 1.6677 - val_loss: 1.6671\n",
      "Epoch 16/100\n",
      "24781/24781 - 18s - loss: 1.6666 - val_loss: 1.6665\n",
      "Epoch 17/100\n",
      "24781/24781 - 18s - loss: 1.6572 - val_loss: 1.6349\n",
      "Epoch 18/100\n",
      "24781/24781 - 19s - loss: 1.6568 - val_loss: 1.6565\n",
      "Epoch 19/100\n",
      "24781/24781 - 18s - loss: 1.6481 - val_loss: 1.6876\n",
      "Epoch 20/100\n",
      "24781/24781 - 18s - loss: 1.6488 - val_loss: 1.6991\n",
      "Epoch 21/100\n",
      "24781/24781 - 18s - loss: 1.6455 - val_loss: 1.6540\n",
      "Epoch 22/100\n",
      "24781/24781 - 18s - loss: 1.6396 - val_loss: 1.6392\n",
      "Epoch 23/100\n",
      "24781/24781 - 17s - loss: 1.6353 - val_loss: 1.6817\n",
      "Epoch 24/100\n",
      "24781/24781 - 17s - loss: 1.6352 - val_loss: 1.6290\n",
      "Epoch 25/100\n",
      "24781/24781 - 17s - loss: 1.6299 - val_loss: 1.6370\n",
      "Epoch 26/100\n",
      "24781/24781 - 18s - loss: 1.6250 - val_loss: 1.6260\n",
      "Epoch 27/100\n",
      "24781/24781 - 17s - loss: 1.6235 - val_loss: 1.6485\n",
      "Epoch 28/100\n",
      "24781/24781 - 17s - loss: 1.6212 - val_loss: 1.6432\n",
      "Epoch 29/100\n",
      "24781/24781 - 17s - loss: 1.6182 - val_loss: 1.6297\n",
      "Epoch 30/100\n",
      "24781/24781 - 18s - loss: 1.6177 - val_loss: 1.6493\n",
      "Epoch 31/100\n",
      "24781/24781 - 17s - loss: 1.6150 - val_loss: 1.6354\n",
      "Epoch 32/100\n",
      "24781/24781 - 17s - loss: 1.6096 - val_loss: 1.6369\n",
      "Epoch 33/100\n",
      "24781/24781 - 17s - loss: 1.6103 - val_loss: 1.6482\n",
      "Epoch 34/100\n",
      "24781/24781 - 17s - loss: 1.6102 - val_loss: 1.6228\n",
      "Epoch 35/100\n",
      "24781/24781 - 17s - loss: 1.6056 - val_loss: 1.6262\n",
      "Epoch 36/100\n",
      "24781/24781 - 17s - loss: 1.6038 - val_loss: 1.6180\n",
      "Epoch 37/100\n",
      "24781/24781 - 17s - loss: 1.6041 - val_loss: 1.6202\n",
      "Epoch 38/100\n",
      "24781/24781 - 17s - loss: 1.6041 - val_loss: 1.6407\n",
      "Epoch 39/100\n",
      "24781/24781 - 17s - loss: 1.5978 - val_loss: 1.6290\n",
      "Epoch 40/100\n",
      "24781/24781 - 17s - loss: 1.5997 - val_loss: 1.6246\n",
      "Epoch 41/100\n",
      "24781/24781 - 17s - loss: 1.5973 - val_loss: 1.6378\n",
      "Epoch 42/100\n",
      "24781/24781 - 17s - loss: 1.5946 - val_loss: 1.6214\n",
      "Epoch 43/100\n",
      "24781/24781 - 17s - loss: 1.5944 - val_loss: 1.6204\n",
      "Epoch 44/100\n",
      "24781/24781 - 17s - loss: 1.5939 - val_loss: 1.6251\n",
      "Epoch 45/100\n",
      "24781/24781 - 17s - loss: 1.5937 - val_loss: 1.6566\n",
      "Epoch 46/100\n",
      "24781/24781 - 17s - loss: 1.5919 - val_loss: 1.6192\n",
      "Epoch 47/100\n",
      "24781/24781 - 17s - loss: 1.5894 - val_loss: 1.6176\n",
      "Epoch 48/100\n",
      "24781/24781 - 17s - loss: 1.5894 - val_loss: 1.6372\n",
      "Epoch 49/100\n",
      "24781/24781 - 17s - loss: 1.5890 - val_loss: 1.6213\n",
      "Epoch 50/100\n",
      "24781/24781 - 17s - loss: 1.5861 - val_loss: 1.6360\n",
      "Epoch 51/100\n",
      "24781/24781 - 17s - loss: 1.5860 - val_loss: 1.6349\n",
      "Epoch 52/100\n",
      "24781/24781 - 17s - loss: 1.5832 - val_loss: 1.6157\n",
      "Epoch 53/100\n",
      "24781/24781 - 17s - loss: 1.5813 - val_loss: 1.6155\n",
      "Epoch 54/100\n",
      "24781/24781 - 17s - loss: 1.5809 - val_loss: 1.6444\n",
      "Epoch 55/100\n",
      "24781/24781 - 17s - loss: 1.5814 - val_loss: 1.6265\n",
      "Epoch 56/100\n",
      "24781/24781 - 17s - loss: 1.5771 - val_loss: 1.6189\n",
      "Epoch 57/100\n",
      "24781/24781 - 17s - loss: 1.5809 - val_loss: 1.6223\n",
      "Epoch 58/100\n",
      "24781/24781 - 18s - loss: 1.5763 - val_loss: 1.6356\n",
      "Epoch 59/100\n",
      "24781/24781 - 17s - loss: 1.5767 - val_loss: 1.6122\n",
      "Epoch 60/100\n",
      "24781/24781 - 17s - loss: 1.5761 - val_loss: 1.6344\n",
      "Epoch 61/100\n",
      "24781/24781 - 17s - loss: 1.5764 - val_loss: 1.6292\n",
      "Epoch 62/100\n",
      "24781/24781 - 17s - loss: 1.5720 - val_loss: 1.6143\n",
      "Epoch 63/100\n",
      "24781/24781 - 17s - loss: 1.5729 - val_loss: 1.6265\n",
      "Epoch 64/100\n",
      "24781/24781 - 17s - loss: 1.5718 - val_loss: 1.6141\n",
      "Epoch 65/100\n",
      "24781/24781 - 18s - loss: 1.5737 - val_loss: 1.6262\n",
      "Epoch 66/100\n",
      "24781/24781 - 17s - loss: 1.5727 - val_loss: 1.6196\n",
      "Epoch 67/100\n",
      "24781/24781 - 18s - loss: 1.5726 - val_loss: 1.6200\n",
      "Epoch 68/100\n",
      "24781/24781 - 17s - loss: 1.5704 - val_loss: 1.6115\n",
      "Epoch 69/100\n",
      "24781/24781 - 17s - loss: 1.5697 - val_loss: 1.6170\n",
      "Epoch 70/100\n",
      "24781/24781 - 18s - loss: 1.5697 - val_loss: 1.6174\n",
      "Epoch 71/100\n",
      "24781/24781 - 17s - loss: 1.5676 - val_loss: 1.6197\n",
      "Epoch 72/100\n",
      "24781/24781 - 18s - loss: 1.5676 - val_loss: 1.6167\n",
      "Epoch 73/100\n",
      "24781/24781 - 18s - loss: 1.5651 - val_loss: 1.6476\n",
      "Epoch 74/100\n",
      "24781/24781 - 17s - loss: 1.5669 - val_loss: 1.6127\n",
      "Epoch 75/100\n",
      "24781/24781 - 17s - loss: 1.5662 - val_loss: 1.6515\n",
      "Epoch 76/100\n",
      "24781/24781 - 17s - loss: 1.5649 - val_loss: 1.6149\n",
      "Epoch 77/100\n",
      "24781/24781 - 17s - loss: 1.5658 - val_loss: 1.6255\n",
      "Epoch 78/100\n",
      "24781/24781 - 17s - loss: 1.5631 - val_loss: 1.6118\n",
      "Epoch 79/100\n",
      "24781/24781 - 17s - loss: 1.5629 - val_loss: 1.6155\n",
      "Epoch 80/100\n",
      "24781/24781 - 18s - loss: 1.5633 - val_loss: 1.6179\n",
      "Epoch 81/100\n",
      "24781/24781 - 17s - loss: 1.5604 - val_loss: 1.6145\n",
      "Epoch 82/100\n",
      "24781/24781 - 17s - loss: 1.5617 - val_loss: 1.6098\n",
      "Epoch 83/100\n",
      "24781/24781 - 18s - loss: 1.5604 - val_loss: 1.6174\n",
      "Epoch 84/100\n",
      "24781/24781 - 17s - loss: 1.5603 - val_loss: 1.6222\n",
      "Epoch 85/100\n",
      "24781/24781 - 17s - loss: 1.5600 - val_loss: 1.6140\n",
      "Epoch 86/100\n",
      "24781/24781 - 17s - loss: 1.5596 - val_loss: 1.6262\n",
      "Epoch 87/100\n",
      "24781/24781 - 17s - loss: 1.5591 - val_loss: 1.6175\n",
      "Epoch 88/100\n",
      "24781/24781 - 17s - loss: 1.5581 - val_loss: 1.6180\n",
      "Epoch 89/100\n",
      "24781/24781 - 17s - loss: 1.5588 - val_loss: 1.6137\n",
      "Epoch 90/100\n",
      "24781/24781 - 17s - loss: 1.5569 - val_loss: 1.6164\n",
      "Epoch 91/100\n",
      "24781/24781 - 17s - loss: 1.5569 - val_loss: 1.6073\n",
      "Epoch 92/100\n",
      "24781/24781 - 17s - loss: 1.5577 - val_loss: 1.6141\n",
      "Epoch 93/100\n",
      "24781/24781 - 17s - loss: 1.5552 - val_loss: 1.6138\n",
      "Epoch 94/100\n",
      "24781/24781 - 17s - loss: 1.5573 - val_loss: 1.6211\n",
      "Epoch 95/100\n",
      "24781/24781 - 17s - loss: 1.5545 - val_loss: 1.6115\n",
      "Epoch 96/100\n",
      "24781/24781 - 17s - loss: 1.5560 - val_loss: 1.6128\n",
      "Epoch 97/100\n",
      "24781/24781 - 17s - loss: 1.5541 - val_loss: 1.6222\n",
      "Epoch 98/100\n",
      "24781/24781 - 17s - loss: 1.5516 - val_loss: 1.6136\n",
      "Epoch 99/100\n",
      "24781/24781 - 17s - loss: 1.5541 - val_loss: 1.6180\n",
      "Epoch 100/100\n",
      "24781/24781 - 17s - loss: 1.5520 - val_loss: 1.6172\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], activation=\"relu\", input_dim=X.shape[1]))\n",
    "model.add(Dense(X.shape[1]*.75, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile model: The model is initialized with the Adam optimizer and then it is compiled.\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200))\n",
    "\n",
    "# Patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=10, verbose=2, callbacks=[es])\n",
    "\n",
    "# Calculate predictions\n",
    "PredTestSet = model.predict(X_train)\n",
    "PredValSet = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 0.43\n",
      "Mean squared error = 1.62\n",
      "Median absolute error = 0.07\n",
      "Explain variance score = 0.96\n",
      "R2 score = 0.962\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, PredValSet), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, PredValSet), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, PredValSet), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, PredValSet), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, PredValSet), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Model Accuracy\n",
    "r_squared = r2_score(y_test,PredValSet)\n",
    "#add RMSE,MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9618329784772767, 0.9618301288356635)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared, adjusted_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the below code to see what percent where our predictions fall within 3, 4 or 5 off from actual\n",
    "#As of Sunday night 98.6, 98.0 and 96.8 percent of predictions are +- 3,4 or 5 of actual.\n",
    "#Biggest improvement is that the model gets it exactly right 83% of the time with new features, compared to 70% prior.\n",
    "y_test_vals =np.reshape(y_test,(y_test.shape[0],))\n",
    "Preds =  np.reshape(PredValSet,(PredValSet.shape[0],))\n",
    "compare = pd.DataFrame(np.array([y_test_vals, Preds]))\n",
    "\n",
    "                                 \n",
    "compare = np.transpose(compare)\n",
    "compare.to_csv(r'compare.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7744/7744 [==============================] - 8s 966us/step - loss: 4.2685 - val_loss: 3.1759\n",
      "Epoch 2/100\n",
      "7744/7744 [==============================] - 7s 853us/step - loss: 2.1700 - val_loss: 1.9181\n",
      "Epoch 3/100\n",
      "7744/7744 [==============================] - 7s 966us/step - loss: 1.9178 - val_loss: 1.7796\n",
      "Epoch 4/100\n",
      "7744/7744 [==============================] - 7s 850us/step - loss: 1.8525 - val_loss: 1.7602\n",
      "Epoch 5/100\n",
      "7744/7744 [==============================] - 7s 858us/step - loss: 1.7752 - val_loss: 1.6965\n",
      "Epoch 6/100\n",
      "7744/7744 [==============================] - 7s 862us/step - loss: 1.7296 - val_loss: 1.8170\n",
      "Epoch 7/100\n",
      "7744/7744 [==============================] - 7s 854us/step - loss: 1.7314 - val_loss: 1.7008\n",
      "Epoch 8/100\n",
      "7744/7744 [==============================] - 6s 838us/step - loss: 1.7083 - val_loss: 1.7430\n",
      "Epoch 9/100\n",
      "7744/7744 [==============================] - 6s 832us/step - loss: 1.7942 - val_loss: 1.6691\n",
      "Epoch 10/100\n",
      "7744/7744 [==============================] - 6s 835us/step - loss: 1.6819 - val_loss: 1.6608\n",
      "Epoch 11/100\n",
      "7744/7744 [==============================] - 6s 834us/step - loss: 1.7230 - val_loss: 1.6853\n",
      "Epoch 12/100\n",
      "7744/7744 [==============================] - 6s 833us/step - loss: 1.6910 - val_loss: 1.6619\n",
      "Epoch 13/100\n",
      "7744/7744 [==============================] - 6s 833us/step - loss: 1.6724 - val_loss: 1.8242\n",
      "Epoch 14/100\n",
      "7744/7744 [==============================] - 6s 833us/step - loss: 1.6596 - val_loss: 1.7059\n",
      "Epoch 15/100\n",
      "7744/7744 [==============================] - 7s 856us/step - loss: 1.6424 - val_loss: 1.6659\n",
      "Epoch 16/100\n",
      "7744/7744 [==============================] - 7s 842us/step - loss: 1.6821 - val_loss: 1.7087\n",
      "Epoch 17/100\n",
      "7744/7744 [==============================] - 7s 850us/step - loss: 1.6523 - val_loss: 1.7320\n",
      "Epoch 18/100\n",
      "7744/7744 [==============================] - 7s 863us/step - loss: 1.6417 - val_loss: 1.6594\n",
      "Epoch 19/100\n",
      "7744/7744 [==============================] - 7s 872us/step - loss: 1.6436 - val_loss: 1.6800\n",
      "Epoch 20/100\n",
      "7744/7744 [==============================] - 7s 853us/step - loss: 1.6388 - val_loss: 1.6782\n",
      "Epoch 21/100\n",
      "7744/7744 [==============================] - 7s 856us/step - loss: 1.6745 - val_loss: 1.7003\n",
      "Epoch 22/100\n",
      "7744/7744 [==============================] - 7s 840us/step - loss: 1.6172 - val_loss: 1.6638\n",
      "Epoch 23/100\n",
      "7744/7744 [==============================] - 7s 867us/step - loss: 1.6456 - val_loss: 1.6359\n",
      "Epoch 24/100\n",
      "7744/7744 [==============================] - 7s 862us/step - loss: 1.6100 - val_loss: 1.6743\n",
      "Epoch 25/100\n",
      "7744/7744 [==============================] - 7s 866us/step - loss: 1.6039 - val_loss: 1.6506\n",
      "Epoch 26/100\n",
      "7744/7744 [==============================] - 7s 868us/step - loss: 1.6038 - val_loss: 1.6374\n",
      "Epoch 27/100\n",
      "7744/7744 [==============================] - 7s 855us/step - loss: 1.5546 - val_loss: 1.6437\n",
      "Epoch 28/100\n",
      "7744/7744 [==============================] - 7s 863us/step - loss: 1.5924 - val_loss: 1.6663\n",
      "Epoch 29/100\n",
      "7744/7744 [==============================] - 7s 840us/step - loss: 1.5545 - val_loss: 1.6930\n",
      "Epoch 30/100\n",
      "7744/7744 [==============================] - 7s 852us/step - loss: 1.6064 - val_loss: 1.6445\n",
      "Epoch 31/100\n",
      "7744/7744 [==============================] - 7s 857us/step - loss: 1.5339 - val_loss: 1.6310\n",
      "Epoch 32/100\n",
      "7744/7744 [==============================] - 7s 844us/step - loss: 1.5627 - val_loss: 1.6615\n",
      "Epoch 33/100\n",
      "7744/7744 [==============================] - 7s 844us/step - loss: 1.5885 - val_loss: 1.6767\n",
      "Epoch 34/100\n",
      "7744/7744 [==============================] - 7s 855us/step - loss: 1.5982 - val_loss: 1.6386\n",
      "Epoch 35/100\n",
      "7744/7744 [==============================] - 7s 853us/step - loss: 1.5940 - val_loss: 1.6368\n",
      "Epoch 36/100\n",
      "7744/7744 [==============================] - 7s 869us/step - loss: 1.5726 - val_loss: 1.6596\n",
      "Epoch 37/100\n",
      "7744/7744 [==============================] - 7s 868us/step - loss: 1.5983 - val_loss: 1.6294\n",
      "Epoch 38/100\n",
      "7744/7744 [==============================] - 7s 844us/step - loss: 1.5585 - val_loss: 1.6259\n",
      "Epoch 39/100\n",
      "7744/7744 [==============================] - 7s 877us/step - loss: 1.5735 - val_loss: 1.6620\n",
      "Epoch 40/100\n",
      "7744/7744 [==============================] - 7s 888us/step - loss: 1.5653 - val_loss: 1.6492\n",
      "Epoch 41/100\n",
      "7744/7744 [==============================] - 7s 865us/step - loss: 1.5370 - val_loss: 1.6444\n",
      "Epoch 42/100\n",
      "7744/7744 [==============================] - 7s 869us/step - loss: 1.5776 - val_loss: 1.6331\n",
      "Epoch 43/100\n",
      "7744/7744 [==============================] - 7s 867us/step - loss: 1.5743 - val_loss: 1.6789\n",
      "Epoch 44/100\n",
      "7744/7744 [==============================] - 7s 864us/step - loss: 1.5402 - val_loss: 1.6429\n",
      "Epoch 45/100\n",
      "7744/7744 [==============================] - 7s 889us/step - loss: 1.5412 - val_loss: 1.6455\n",
      "Epoch 46/100\n",
      "7744/7744 [==============================] - 7s 910us/step - loss: 1.5232 - val_loss: 1.6550\n",
      "Epoch 47/100\n",
      "7744/7744 [==============================] - 7s 875us/step - loss: 1.5425 - val_loss: 1.6270\n",
      "Epoch 48/100\n",
      "7744/7744 [==============================] - 7s 854us/step - loss: 1.5140 - val_loss: 1.6326\n",
      "Epoch 49/100\n",
      "7744/7744 [==============================] - 7s 886us/step - loss: 1.5547 - val_loss: 1.6243\n",
      "Epoch 50/100\n",
      "7744/7744 [==============================] - 7s 876us/step - loss: 1.5308 - val_loss: 1.6570\n",
      "Epoch 51/100\n",
      "7744/7744 [==============================] - 7s 866us/step - loss: 1.5075 - val_loss: 1.6347\n",
      "Epoch 52/100\n",
      "7744/7744 [==============================] - 7s 845us/step - loss: 1.5096 - val_loss: 1.6730\n",
      "Epoch 53/100\n",
      "7744/7744 [==============================] - 7s 843us/step - loss: 1.5174 - val_loss: 1.6376\n",
      "Epoch 54/100\n",
      "7744/7744 [==============================] - 6s 839us/step - loss: 1.5563 - val_loss: 1.6324\n",
      "Epoch 55/100\n",
      "7744/7744 [==============================] - 7s 841us/step - loss: 1.5277 - val_loss: 1.6467\n",
      "Epoch 56/100\n",
      "7744/7744 [==============================] - 7s 844us/step - loss: 1.5224 - val_loss: 1.6837\n",
      "Epoch 57/100\n",
      "7744/7744 [==============================] - 7s 847us/step - loss: 1.5253 - val_loss: 1.6765\n",
      "Epoch 58/100\n",
      "7744/7744 [==============================] - 7s 851us/step - loss: 1.5145 - val_loss: 1.6852\n",
      "Epoch 59/100\n",
      "7744/7744 [==============================] - 7s 877us/step - loss: 1.5071 - val_loss: 1.6589\n",
      "Epoch 60/100\n",
      "7744/7744 [==============================] - 7s 890us/step - loss: 1.5268 - val_loss: 1.6633\n",
      "Epoch 61/100\n",
      "7744/7744 [==============================] - 7s 896us/step - loss: 1.5241 - val_loss: 1.6601\n",
      "Epoch 62/100\n",
      "7744/7744 [==============================] - 7s 882us/step - loss: 1.5437 - val_loss: 1.7225\n",
      "Epoch 63/100\n",
      "7744/7744 [==============================] - 7s 861us/step - loss: 1.5289 - val_loss: 1.6552\n",
      "Epoch 64/100\n",
      "7744/7744 [==============================] - 6s 829us/step - loss: 1.4952 - val_loss: 1.6713\n",
      "Epoch 65/100\n",
      "7744/7744 [==============================] - 7s 845us/step - loss: 1.5216 - val_loss: 1.7006\n",
      "Epoch 66/100\n",
      "7744/7744 [==============================] - 7s 852us/step - loss: 1.5212 - val_loss: 1.6389\n",
      "Epoch 67/100\n",
      "7744/7744 [==============================] - 7s 851us/step - loss: 1.5004 - val_loss: 1.6750\n",
      "Epoch 68/100\n",
      "7744/7744 [==============================] - 7s 871us/step - loss: 1.5284 - val_loss: 1.6441\n",
      "Epoch 69/100\n",
      "7744/7744 [==============================] - 7s 896us/step - loss: 1.5143 - val_loss: 1.7049\n",
      "Epoch 70/100\n",
      "7744/7744 [==============================] - 7s 883us/step - loss: 1.5170 - val_loss: 1.6844\n",
      "Epoch 71/100\n",
      "7744/7744 [==============================] - 7s 867us/step - loss: 1.4940 - val_loss: 1.6435\n",
      "Epoch 72/100\n",
      "7744/7744 [==============================] - 7s 869us/step - loss: 1.4734 - val_loss: 1.6482\n",
      "Epoch 73/100\n",
      "7744/7744 [==============================] - 7s 890us/step - loss: 1.5040 - val_loss: 1.6835\n",
      "Epoch 74/100\n",
      "7744/7744 [==============================] - 8s 972us/step - loss: 1.5382 - val_loss: 1.6827\n",
      "Epoch 75/100\n",
      "7744/7744 [==============================] - 8s 990us/step - loss: 1.5061 - val_loss: 1.6728\n",
      "Epoch 76/100\n",
      "7744/7744 [==============================] - 7s 929us/step - loss: 1.4847 - val_loss: 1.7305\n",
      "Epoch 77/100\n",
      "7744/7744 [==============================] - 7s 883us/step - loss: 1.4911 - val_loss: 1.7064\n",
      "Epoch 78/100\n",
      "7744/7744 [==============================] - 7s 872us/step - loss: 1.5297 - val_loss: 1.6772\n",
      "Epoch 79/100\n",
      "7744/7744 [==============================] - 7s 860us/step - loss: 1.5056 - val_loss: 1.6752\n",
      "Epoch 80/100\n",
      "7744/7744 [==============================] - 7s 849us/step - loss: 1.5058 - val_loss: 1.7127\n",
      "Epoch 81/100\n",
      "7744/7744 [==============================] - 7s 858us/step - loss: 1.4628 - val_loss: 1.6645\n",
      "Epoch 82/100\n",
      "7744/7744 [==============================] - 7s 852us/step - loss: 1.4993 - val_loss: 1.6961\n",
      "Epoch 83/100\n",
      "7744/7744 [==============================] - 7s 880us/step - loss: 1.4838 - val_loss: 1.6957\n",
      "Epoch 84/100\n",
      "7744/7744 [==============================] - 7s 944us/step - loss: 1.4805 - val_loss: 1.6607\n",
      "Epoch 85/100\n",
      "7744/7744 [==============================] - 7s 869us/step - loss: 1.4720 - val_loss: 1.6621\n",
      "Epoch 86/100\n",
      "7744/7744 [==============================] - 7s 917us/step - loss: 1.4748 - val_loss: 1.6983\n",
      "Epoch 87/100\n",
      "7744/7744 [==============================] - 6s 830us/step - loss: 1.4804 - val_loss: 1.6848\n",
      "Epoch 88/100\n",
      "7744/7744 [==============================] - 6s 825us/step - loss: 1.4789 - val_loss: 1.7326\n",
      "Epoch 89/100\n",
      "7744/7744 [==============================] - 6s 823us/step - loss: 1.5123 - val_loss: 1.6779\n",
      "Epoch 90/100\n",
      "7744/7744 [==============================] - 6s 827us/step - loss: 1.4824 - val_loss: 1.7394\n",
      "Epoch 91/100\n",
      "7744/7744 [==============================] - 6s 838us/step - loss: 1.4853 - val_loss: 1.6981\n",
      "Epoch 92/100\n",
      "7744/7744 [==============================] - 6s 823us/step - loss: 1.5081 - val_loss: 1.7147\n",
      "Epoch 93/100\n",
      "7744/7744 [==============================] - 6s 823us/step - loss: 1.4618 - val_loss: 1.7139\n",
      "Epoch 94/100\n",
      "7744/7744 [==============================] - 6s 828us/step - loss: 1.4723 - val_loss: 1.6980\n",
      "Epoch 95/100\n",
      "7744/7744 [==============================] - 6s 833us/step - loss: 1.4727 - val_loss: 1.7960\n",
      "Epoch 96/100\n",
      "7744/7744 [==============================] - 6s 834us/step - loss: 1.4953 - val_loss: 1.7196\n",
      "Epoch 97/100\n",
      "7744/7744 [==============================] - 6s 825us/step - loss: 1.4902 - val_loss: 1.7369\n",
      "Epoch 98/100\n",
      "7744/7744 [==============================] - 6s 828us/step - loss: 1.5239 - val_loss: 1.7563\n",
      "Epoch 99/100\n",
      "7744/7744 [==============================] - 6s 825us/step - loss: 1.4481 - val_loss: 1.6702\n",
      "Epoch 100/100\n",
      "7744/7744 [==============================] - 6s 826us/step - loss: 1.5207 - val_loss: 1.7415\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4659\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4670\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.4667\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.7917\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5451\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 6.2814\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.9144\n",
      "7744/7744 [==============================] - 3s 353us/step - loss: 7.4651\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 2.5072\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4819\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 3.4445\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4735\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 12.0383\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 2.6256\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.9279\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.6812\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.6847\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.5609\n",
      "7744/7744 [==============================] - 3s 361us/step - loss: 1.7941\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.5424\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.4663\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4799\n",
      "7744/7744 [==============================] - 3s 352us/step - loss: 1.4732\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4759\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.5957\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.5988\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4895\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4886\n",
      "7744/7744 [==============================] - 3s 353us/step - loss: 1.6865\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.5170\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.6117\n",
      "7744/7744 [==============================] - 3s 363us/step - loss: 1.7351\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.4829\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 2.0603\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4980\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5889\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.4781\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4696\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4675\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4658\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.7971\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.5431\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 6.2229\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.9114\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 7.4230\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 2.5005\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4859\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 3.4313\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4736\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 12.0820\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 2.6146\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.9596\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.6741\n",
      "7744/7744 [==============================] - 3s 353us/step - loss: 1.6924\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5629\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.7985\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.5467\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4663\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4782\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.4725\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4757\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.5971\n",
      "7744/7744 [==============================] - 3s 379us/step - loss: 1.6007\n",
      "7744/7744 [==============================] - 3s 369us/step - loss: 1.4908\n",
      "7744/7744 [==============================] - 3s 362us/step - loss: 1.4896\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6790\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.5163\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6083\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.7317\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.4828\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 2.0914\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4946\n",
      "7744/7744 [==============================] - 3s 374us/step - loss: 1.5913\n",
      "7744/7744 [==============================] - 3s 401us/step - loss: 1.4774\n",
      "7744/7744 [==============================] - 3s 393us/step - loss: 1.4692\n",
      "7744/7744 [==============================] - 3s 384us/step - loss: 1.4666\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.7874\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.5447\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 6.2362\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.9139\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 7.4790\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 2.5199\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4847\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 3.5310\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4716\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 12.0842\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 2.6315\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.9409\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6733\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.6861\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.5628\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.7881\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5464\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4665\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4779\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4733\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4765\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.5999\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.6023\n",
      "7744/7744 [==============================] - 3s 367us/step - loss: 1.4928\n",
      "7744/7744 [==============================] - 3s 363us/step - loss: 1.4918\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.6838\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.5131\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6134\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.7268\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.4831\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 2.0698\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4969\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5904\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 1.4766\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4693\n",
      "7744/7744 [==============================] - 3s 364us/step - loss: 1.4670\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4665\n",
      "7744/7744 [==============================] - 3s 362us/step - loss: 1.7972\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.5437\n",
      "7744/7744 [==============================] - 3s 366us/step - loss: 6.2412\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.9110\n",
      "7744/7744 [==============================] - 3s 353us/step - loss: 7.3998\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 2.5482\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4835\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 3.4410\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4731\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 12.0187\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 2.6112\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.9500\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.6841\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6960\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.5609\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.7868\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.5412\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4662\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4805\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4704\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4761\n",
      "7744/7744 [==============================] - 3s 366us/step - loss: 1.6058\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.6021\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4893\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4901\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6845\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.5166\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.6157\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.7244\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 1.4823\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 2.0640\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4944\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.5892\n",
      "7744/7744 [==============================] - 3s 355us/step - loss: 1.4781\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4698\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.4668\n",
      "7744/7744 [==============================] - 3s 365us/step - loss: 1.4663\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.7924\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.5435\n",
      "7744/7744 [==============================] - 3s 360us/step - loss: 6.2634\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.9073\n",
      "7744/7744 [==============================] - 3s 367us/step - loss: 7.4051\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 2.5080\n",
      "7744/7744 [==============================] - 3s 363us/step - loss: 1.4859\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 3.5007\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4741\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 12.0330\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 2.6265\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.9469\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6861\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.6839\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.5641\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.7884\n",
      "7744/7744 [==============================] - 3s 361us/step - loss: 1.5432\n",
      "7744/7744 [==============================] - 3s 365us/step - loss: 1.4664\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4770\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.4708\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4754\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.6043\n",
      "7744/7744 [==============================] - 3s 357us/step - loss: 1.6022\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4932\n",
      "7744/7744 [==============================] - 3s 356us/step - loss: 1.4923\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.6841\n",
      "7744/7744 [==============================] - 3s 361us/step - loss: 1.5110\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.6093\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.7178\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4821\n",
      "7744/7744 [==============================] - 3s 354us/step - loss: 2.1031\n",
      "7744/7744 [==============================] - 3s 362us/step - loss: 1.4944\n",
      "7744/7744 [==============================] - 3s 364us/step - loss: 1.5904\n",
      "7744/7744 [==============================] - 3s 359us/step - loss: 1.4784\n",
      "7744/7744 [==============================] - 3s 358us/step - loss: 1.4693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                10.5854\n",
       "                \n",
       "                    &plusmn; 0.0536\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                OriginalPropOrderQty\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                5.9685\n",
       "                \n",
       "                    &plusmn; 0.0639\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BaseOrder\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.53%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                4.7832\n",
       "                \n",
       "                    &plusmn; 0.0416\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ForecastedScans\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                2.0039\n",
       "                \n",
       "                    &plusmn; 0.0782\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                TrueUpAdjQty\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                1.1560\n",
       "                \n",
       "                    &plusmn; 0.0153\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ConversionUnits\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                1.0509\n",
       "                \n",
       "                    &plusmn; 0.0338\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                SlowMoving\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.6119\n",
       "                \n",
       "                    &plusmn; 0.0333\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                days_last_delivered\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.71%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.4792\n",
       "                \n",
       "                    &plusmn; 0.0210\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                MaxDeliveredQty\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.4457\n",
       "                \n",
       "                    &plusmn; 0.0051\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                WeightData\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.3273\n",
       "                \n",
       "                    &plusmn; 0.0074\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                InventoryChange\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.3253\n",
       "                \n",
       "                    &plusmn; 0.0089\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                OneOrderAgo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.2613\n",
       "                \n",
       "                    &plusmn; 0.0119\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                avg_invent_change\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.66%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.2228\n",
       "                \n",
       "                    &plusmn; 0.0095\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                TwoOrderAgo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.68%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.2177\n",
       "                \n",
       "                    &plusmn; 0.0049\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                OGvsModifiedOG\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.2139\n",
       "                \n",
       "                    &plusmn; 0.0104\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ConversionResidual\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1458\n",
       "                \n",
       "                    &plusmn; 0.0054\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                daily_invntchange\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.05%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1353\n",
       "                \n",
       "                    &plusmn; 0.0027\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                originalqty_avg\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.06%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1347\n",
       "                \n",
       "                    &plusmn; 0.0079\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                order_maxorder\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1242\n",
       "                \n",
       "                    &plusmn; 0.0018\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                forecasted_origprop_ratio\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0965\n",
       "                \n",
       "                    &plusmn; 0.0025\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                QtyShippedTwoOrderAgo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.25%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 17 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determine variable importance in neural network model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[1], activation=\"relu\", input_dim=X.shape[1]))\n",
    "    model.add(Dense(36, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200)) \n",
    "    return model\n",
    "\n",
    "my_model = KerasRegressor(build_fn=base_model)    \n",
    "my_model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=100)\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e5b8e0b505b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Best GBM Parameters:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbm_rs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#gbm_scores_train = gbm_rs.predict_proba(X_train)[:, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbm = GradientBoostingRegressor(random_state=314)\n",
    "\n",
    "param_grid = {'n_estimators': [100,300,500,1000], \n",
    "              'learning_rate': [.025, 0.05, .25, 0.5],\n",
    "              'criterion': [‘friedman_mse’, ‘mse’, ‘mae’],\n",
    "             'loss': [‘ls’, ‘lad’, ‘huber’, ‘quantile’]}\n",
    "gbm_rs = RandomizedSearchCV(gbm, param_grid, cv=3, n_iter=100, n_jobs=-1, random_state=314)\n",
    "\n",
    "gbm.fit(X_train, y_train)\n",
    "print ('Best GBM Parameters:', gbm_rs.best_params_)\n",
    "\n",
    "#gbm_scores_train = gbm_rs.predict_proba(X_train)[:, 1]\n",
    "#gbm_scores_test = gbm_rs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "gbm_scores_train = gbm_rs.predict(X_train)\n",
    "gbm_scores_test = gbm_rs.predict(X_test)\n",
    "\n",
    "#gbm_fpr_train, gbm_tpr_train, _ = roc_curve(y_train, gbm_scores_train)\n",
    "#gbm_fpr_test, gbm_tpr_test, _ = roc_curve(y_test, gbm_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liz/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b6132a5273e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgb_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbrt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m314\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgb_rs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt=GradientBoostingRegressor(n_estimators=100, learning_rate=.025,criterion='friedman_mse')\n",
    "\n",
    "param_grid = {'n_estimators': [100,300,500,1000], \n",
    "              'learning_rate': [.025,.05,.25,.5],\n",
    "              'criterion': ['friedman_mse','mse','mae'],\n",
    "              'loss': ['ls','lad','huber','quantile']}\n",
    "\n",
    "gb_rs = RandomizedSearchCV(gbrt,param_grid,n_jobs=-1,random_state=314)\n",
    "\n",
    "gb_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#y_pred=gbrt.predict(X_test)\n",
    "#gbm_scores_test = gbm_rs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-ff357d8ec772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgbrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'friedman_mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgb_rs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt=GradientBoostingRegressor(n_estimators=100, learning_rate=.1,criterion='friedman_mse',loss='ls')\n",
    "\n",
    "gb_rs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#y_pred=gbrt.predict(X_test)\n",
    "#gbm_scores_test = gbm_rs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gbrt.predict(X_test)\n",
    "#gb_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Model R2\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_pred), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_pred), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_pred), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9170284271240234"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594636951717443"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best is batch size of 10 with 100 epics\n",
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(42, activation=\"relu\", input_dim=X.shape[1]))\n",
    "    #model.add(Dense(36, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200))\n",
    "    return model\n",
    "\n",
    "seed=314\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "batch_size = [10,100]\n",
    "epochs = [10,100]\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
